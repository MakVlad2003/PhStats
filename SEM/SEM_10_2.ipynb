{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2MYXOwSSKTS"
   },
   "source": [
    "# Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voNdfERLSKTb"
   },
   "source": [
    "Цель этого ноутбука &mdash; знакомство со случайными лесами, с их параметрами и свойствами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hda-RcjNSKTd"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "sns.set(context='poster')\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO-Bx5ZYSKTe"
   },
   "source": [
    "### Основные параметры\n",
    "\n",
    "**Реализации: `RandomForestClassifier`, `RandomForestRegressor`**\n",
    "\n",
    "Набор гиперпараметров случайного леса очень похож на набор гиперпараметров решающего дерева. Основным отличием является наличие у случайного леса параметра `n_estimators`, задающего количество решающих деревьев, используемых для получения предсказаний. Это **основной гиперпараметр** для случайного леса.\n",
    "\n",
    "Напомним главные гиперпараметры решающего дерева, которые также имеются у случайного леса.\n",
    "\n",
    "* `criterion` — критерий информативности, по которому происходит разбиение вершины дерева.\n",
    "\n",
    "* `max_depth` — ограничение на глубину каждого дерева в лесе.\n",
    "\n",
    "* `min_samples_split` — минимальное количество элементов обучающей выборки в вершине дерева, чтобы её можно было разбивать.\n",
    "\n",
    "* `min_samples_leaf` — минимальное количество элементов обучающей выборке в листовой вершине.\n",
    "\n",
    "* `splitter` — способ разбиения вершины каждого решающего дерева. Есть 2 возможных варианта: `best` и `random`. В первом случае рассматриваются все возможные способы разбить вершину дерева на две и берётся тот из них, значение критерия для которого оптимально. При `splitter=random` берётся несколько случайных возможных разбиений и среди них выбирается то, значение критерия для которого оптимально.\n",
    "\n",
    "* `max_features` — максимальное количество признаков, которые могут быть перебраны при разбиении вершины дерева. Перед каждым разбиением дерева генерируется выборка из `min(k, max_features)` случайных признаков (`k` — количество признаков в датасете) и только эти признаки рассматриваются как разделяющие в данной вершине. <br>\n",
    "Этот признак может принимать\n",
    "    * целочисленное значение — число признаков,\n",
    "    * вещественное значение — доля признаков,\n",
    "    * `None` — все признаки,\n",
    "    * `\"auto\"` — квадратный корень от числа всех признаков (по умолчанию),\n",
    "    * `\"sqrt\"` — квадратный корень от числа всех признаков,\n",
    "    * `\"log2\"` — двоичный логарифм от числа всех признаков.\n",
    "    \n",
    "* `min_impurity_decrease` — минимальное значение уменьшения взвешенного критерия неопределенности (`impurity`), чтобы можно было разбить выборку в данной вершине.\n",
    "\n",
    "О других гиперпараметрах случайного леса можно почитать в <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">документации</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsucdAW4SKT0"
   },
   "source": [
    "## Решение задачи классификации с помощью Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_3yFe_TSKT0"
   },
   "source": [
    "Возьмем классический датасет Breast Canser для определения доброкачественной или злокачественной опухоли. Его можно скачать с помощью `sklearn`, а дополнительную информацию о переменных можно почитать <a href=\"https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+dicancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.targetagnostic\">тут</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh7VA-GOSKT1"
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s55tsLQ4SKT1"
   },
   "source": [
    "### Зависимость точности классификации от значений гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exxo87FrSKT2"
   },
   "source": [
    "Разобьём данные на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UH12H3xNSKT2"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLS9O9g7SKT2"
   },
   "source": [
    "Для начала попробуем оценить оптимальное количество решающих деревьев в лесе, взяв значения всех остальных параметров по умолчанию. Построим график зависимости `accuracy` от `n_estimators` на обучающей и на тестовой выборках. В большинстве случаев, значение `n_estimators` берут в диапазоне от 10 до 100. Но здесь мы рассмотрим более широкий набор значений — от 1 до 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_18ex0NHAzt"
   },
   "outputs": [],
   "source": [
    "n_estimators_values = np.arange(1, 201)\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for n in n_estimators_values:\n",
    "    clf = RandomForestClassifier(n_estimators = n, random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_accuracies.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 931
    },
    "id": "IlrnArRCJXsI",
    "outputId": "cb66f2eb-2e1f-46a1-cb60-5fbdcb6ea423"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(n_estimators_values, train_accuracies, label = 'Обучающая выборка')\n",
    "plt.plot(n_estimators_values, test_accuracies, label = 'Тестовая выборка')\n",
    "\n",
    "plt.title('Зависимость точности от количества решающих деревьев')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('Точность')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usYblJmFSKT4"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "1. На обучающей выборке точность увеличивается с увеличением числа деревьев, но после примерно 70 деревьев она выходит на плато. Это говорит о том, что добавление дополнительных деревьев не приводит к существенному увеличению точности на обучающей выборке.\n",
    "\n",
    "2. На тестовой выборке также наблюдается увеличение точности с увеличением числа деревьев, но она выходит на плато примерно в районе значения 0.965. Это означает, что дальнейшее увеличение числа деревьев может не приносить значительного прироста точности на тестовой выборке из-за переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0fOupcdHAzt"
   },
   "source": [
    "В теории, при предположении, что все решающие деревья в лесе независимы между собой, должно получаться, что при увеличении числа случайных решающих деревьев в лесе дисперсия предсказания монотонно снижается, а точность монотонно повышается. Однако из-за того, что на практике решающие деревья попарно скоррелированны, такой эффект наблюдается лишь до некоторого значения `n_estimators`, а затем значительных изменений не происходит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3yVz_vZSKT5"
   },
   "source": [
    "Определите из графика оптимальное значение `n_estimators` и используйте это значение во всех последующих экспериментах с данным датасетом. Построим график зависимости `accuracy` от `min_samples_leaf` на обучающей и на тестовой выборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "VGeGzEYWHAzu",
    "outputId": "23f5202c-1feb-42de-a804-0b0c8ec6c73e"
   },
   "outputs": [],
   "source": [
    "optimal_n_estimators = n_estimators_values[np.argmax(test_accuracies)]\n",
    "print(\"Оптимальное количество деревьев:\", optimal_n_estimators)\n",
    "\n",
    "optimal_clf = RandomForestClassifier(n_estimators = optimal_n_estimators, random_state = 42)\n",
    "optimal_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvDfA764LElD"
   },
   "outputs": [],
   "source": [
    "min_samples_leaf_values = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "train_accuracies_leaf = []\n",
    "test_accuracies_leaf = []\n",
    "\n",
    "for min_samples_leaf in min_samples_leaf_values:\n",
    "    clf = RandomForestClassifier(n_estimators = optimal_n_estimators, min_samples_leaf = min_samples_leaf, random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    train_accuracies_leaf.append(train_accuracy)\n",
    "\n",
    "    test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_accuracies_leaf.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 931
    },
    "id": "xsAivmk2LM7t",
    "outputId": "4171932b-b1de-4ec4-c407-af90b9875446"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(min_samples_leaf_values, train_accuracies_leaf, label = 'Обучающая выборка')\n",
    "plt.plot(min_samples_leaf_values, test_accuracies_leaf, label = 'Тестовая выборка')\n",
    "\n",
    "plt.title('Зависимость точности от min_samples_leaf')\n",
    "plt.xlabel('min_samples_leaf')\n",
    "plt.ylabel('Точность')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u306_ZIDSKT7"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "1. Точность на обучающей выборке убывает с увеличением min_samples_leaf до примерно значения 15. Это может указывать на то, что при увеличении минимального количества элементов в листе дерева модель становится более консервативной и менее склонной к переобучению, что снижает точность на обучающей выборке.\n",
    "\n",
    "2. Однако на графике точности на тестовой выборке после перелома, примерно в том же месте, начинается рост точности с увеличением min_samples_leaf. Это может свидетельствовать о том, что при увеличении значения min_samples_leaf модель становится менее склонной к переобучению и лучше обобщает данные, что приводит к увеличению точности на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9irpO8V4HAzu"
   },
   "source": [
    "Теперь повторим повторим эксперимент для параметра `min_samples_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgTgZ0hiHAzu"
   },
   "outputs": [],
   "source": [
    "min_samples_split_values = [2, 5, 10, 20, 50]\n",
    "\n",
    "train_accuracies_split = []\n",
    "test_accuracies_split = []\n",
    "\n",
    "for min_samples_split in min_samples_split_values:\n",
    "    clf = RandomForestClassifier(n_estimators = optimal_n_estimators, min_samples_split = min_samples_split, random_state = 42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_pred = clf.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    train_accuracies_split.append(train_accuracy)\n",
    "\n",
    "    test_pred = clf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_pred)\n",
    "    test_accuracies_split.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "87AX-4YmNCPW",
    "outputId": "cb66275b-80a9-4ac0-9bc1-863398aab4e2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "\n",
    "plt.plot(min_samples_split_values, train_accuracies_split, label = 'Обучающая выборка')\n",
    "plt.plot(min_samples_split_values, test_accuracies_split, label = 'Тестовая выборка')\n",
    "\n",
    "plt.title('Зависимость точности от min_samples_split с использованием оптимального количества деревьев')\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('Точность')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxsrhGJ5SKT7"
   },
   "source": [
    "**Выводы:**\n",
    "\n",
    "1. Убывающая тенденция на обоих графиках говорит о том, что увеличение min_samples_split создает более простые деревья, что может снизить их способность к обобщению данных.\n",
    "\n",
    "2. Высокая точность на обучающей выборке по сравнению с тестовой указывает на переобучение модели, где она слишком сильно настроена на тренировочные данные и плохо обобщает их на новые данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFlNr87zHAzu"
   },
   "source": [
    "**Общие выводы по всей задаче:**\n",
    "\n",
    "1. Количество деревьев (n_estimators) оказывает значительное влияние на точность модели. Мы определили оптимальное количество деревьев, которое обеспечивает хорошую точность на тестовой выборке, избегая переобучения.\n",
    "\n",
    "2. Гиперпараметры min_samples_leaf и min_samples_split также влияют на точность модели. Увеличение этих параметров может уменьшить переобучение модели, но слишком большие значения могут привести к снижению точности на тестовой выборке.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
